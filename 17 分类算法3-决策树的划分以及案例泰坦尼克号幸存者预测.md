# 决策树的划分依据之一—信息增益

信息增益表示得知特征X的信息而使得类Y的信息不确定性减少的程度。

特征A对训练数据集D的信息增益g(D,A)，定义为集合D的信息熵H(D)与特征A给定条件下D的信息条件熵H(D|A)之差，即公式为：`g(D,A)=H(D)-H(D|A)`

**信息增益计算举例：**

 信贷情况表![preview](https://pic4.zhimg.com/v2-64a9fa21abcbeeb400a5017e00e25f6f_r.jpg) 

结合贷款数据来看公式：

1.信息熵的计算：

 ![[公式]](https://www.zhihu.com/equation?tex=H%28D%29+%3D+-%5Csum_%7Bk+%3D+1%7D%5E%7BK%7D%7B%5Cfrac%7B%7CC_k%7C%7D%7B%7CD%7C%7D%7Dlog_2%5Cfrac%7B%7CC_k%7C%7D%7B%7CD%7C%7D) 

2.条件熵的计算：

 ![[公式]](https://www.zhihu.com/equation?tex=H%28D+%7C+A%29+%3D+%5Csum_%7Bi+%3D+1%7D%5E%7Bn%7D%7B%5Cfrac%7B%7CD_i%7C%7D%7B%7CD%7C%7D%7DH%28D_i%29+%3D+-%5Csum_%7Bi+%3D+1%7D%5E%7Bn%7D%7B%5Cfrac%7B%7CD_i%7C%7D%7B%7CD%7C%7D%7D%5Csum_%7Bk+%3D+1%7D%5E%7BK%7D%7B%5Cfrac%7B%7CD_%7Bik%7D%7C%7D%7B%7CD_i%7C%7D%7Dlog_2%5Cfrac%7B%7CD_%7Bik%7D%7C%7D%7B%7CD_i%7C%7D) 

**注**：Ck表示属于某个类别的样本数

3.对是否通过贷款申请的例子计算决策特征顺序：

首先计算总经验熵：`H(D)=-(9/15)*log(9/15)-6/15=0.971`

然后让`A1`，`A2`，`A3`，`A4`分别表示年龄、有工作、有自己的房子和信贷情况4个特征，则计算出年龄的信息增益：

`g(D,A1)=H(D)-[(5/15)*H(D1)+(5/15)*H(D2)+(5/15)*H(D3)]=0.083`

`H(D1)=-(2/5)log(2/5)-(3/5)log(3/5)`

`H(D2)=-(3/5)log(3/5)-(2/5)log(2/5)`

`H(D3)=-(4/5)log(4/5)-(1/5)log(1/5)`

同理其他的也可以计算出来，`g(D,A2)=0.324`，`g(D,A3)=0.420`，`g(D,A4)=0.363`,相比较来说其中特征`A3`（有自己的房子）的信息增益最大，所以我们把特征`A3`作为决策树的第一判别标准

# 常见决策树使用算法

`ID3`：信息增益  最大的准则

`C4.5`：信息增益比  最大的准则

`CART`：回归树：平方误差  最小；分类树：基尼系数  最小的准则  在`sklearn`可以选择划分的默认原则

# `sklearn`决策树`API`

`class sklearn.tree.DecisionTreeClassifier(criterion='gini',max_depth=None,random_state=None)`

决策树分类器

`criterion`:默认是'`gini`'系数，也可以选择信息增益的熵'entropy'

`max_depth`:树的深度大小

`random_state`:随机数种子

`method`：`decision_path`:返回决策树的路径

# 决策树案例

**泰坦尼克号数据介绍**：

我们提取的数据集中的特征是票的类别、存活、乘坐班、年龄、登陆、`home.dest`、房间、票、船和性别。乘坐班是指乘客班(1,2,3)，是社会经济层的代表；注意其中age数据有缺失。

**泰坦尼克号生存分类模型的建立步骤**

1.`pd`读取数据

2.选择有影响的特征，处理缺失值

3.进行特征工程，`pd`转换字典，特征抽取`x_train.to_dict(orient="records")`

4.决策树估计器流程

5.导出决策树结构：

`API`：`sklearn.tree.export_graphviz`

语法：`export_graphviz(tree_decision,out_file='./datasets/tree.dot',feature_names=['Age', 'Pclass', 'Sex=female', 'Sex=male'])`

该函数能够导出DOT格式

**注**：将dot文件导入`Gvedit`中，就可以看到树的结构了

**代码**：

```python
import pandas as pd
from sklearn.tree import DecisionTreeClassifier,export_graphviz
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction import DictVectorizer
def tree_decision():
    """
    决策树对泰坦尼克号进行预测生死
    :return: None
    """
    # 获取数据
    titanic=pd.read_csv('./datasets/train.csv')
    # 处理数据，找出特征值和目标值
    x = titanic[['Age', 'Pclass','Sex']]
    y = titanic['Survived']
    # 处理缺失值
    x['Age'].fillna(x['Age'].mean(), inplace=True)
    # 分割数据集
    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25)
    # 特征工程处理数据（特征：类别-->one_hot编码）
    dict = DictVectorizer(sparse=False)
    x_train = dict.fit_transform(x_train.to_dict(orient="records"))
    x_test = dict.fit_transform(x_test.to_dict(orient="records"))
    print(dict.get_feature_names())
    # 用决策树进行预测
    tree_decision = DecisionTreeClassifier(max_depth=5)
    tree_decision.fit(x_train, y_train)
    print('预测准确率为：', tree_decision.score(x_test, y_test))
    # 导出决策树的结构
    export_graphviz(tree_decision, out_file='./datasets/tree.dot', feature_names=['年龄', 'Pclass', '女性', '男性'])
    return None


if __name__ =="__main__":
    tree_decision()
```

**运行结果**：

```python
['Age', 'Pclass', 'Sex=female', 'Sex=male']
预测准确率为： 0.8340807174887892
```

# 决策树的优缺点以及改进

**优点**：

简单的理解和解释，树木可视化；

需要很少的数据准备，其他技术通常数据归一化。

**缺点**：

决策树学习者可以创建不能很好地推广数据的过于复杂的树，这被称为过拟合。

**改进**：

减枝cart算法(决策树`API`当中已经实现，随机森林参数调优有相关介绍)；

随机森林。

**注**：企业重要决策，由于决策树很好的分析能力，在决策过程应用较多。

