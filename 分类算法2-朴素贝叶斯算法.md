# 概率基础

朴素贝叶斯算法是通过计算出属于每一类别的概率大小来预估其属于哪个类别(概率最大的那个类别)

联合概率：两个条件同时成立的概率`P(A,B)=P(A)P(B)`

条件概率：A在B发生条件下发生的概率 `P(A1,A2∣B）=P(A1∣B)P(A2∣B)`  (`A1,A2`相互独立)

# 朴素贝叶斯算法

**使用条件**：所有特征条件之间条件独立

**用途**:常用于文档分类(神经网络的效果更好)

## 朴素贝叶斯—贝叶斯公式

P(C∣W)=[P(W∣C)P(C)]/P(W)

注：w为给定文档的特征值(频数统计，预测文档提供)，c为文档类别

P(W)对于同一篇文章是一致的

公式可以理解为：`P(C∣F1,F2，…)=[P(F1,F2,…∣C)P(C)]/P(F1,F2,…)`   其中c可以是不同类别

公式分为三部分：

1.`P(C)`:每个文档类别的概率(某文档类别数/总文档数量)

2.`P(W∣C)`:给定 类别下特征(被预测文档中出现的词)的概率

计算方法：`P(F1∣C)=Ni/N`(训练文档中去计算)

Ni为该`F1`词在C类别所有文档中出现的次数;N为所属类别C下的文档所有词出现的次数

`P(F1,F2,…)`预测文档中每个词的概率

| 特征\统计  | 科技(30篇) | 娱乐(60篇) | 汇总(90篇) |
| ---------- | ---------- | ---------- | ---------- |
| 商场       | 9          | 51         | 60         |
| 影院       | 8          | 56         | 64         |
| 支付宝     | 20         | 15         | 35         |
| 云计算     | 63         | 0          | 63         |
| 汇总(求和) | 100        | 121        | 221        |

现有一篇被预测文档：出现了影院，支付宝，云计算，计算属于科技、娱乐的类别概率？

科技：P(影院，支付宝，云计算|科技)*P(科技)=`8/100x20/100x63/100x30/90`=0.00456109

娱乐：P(影院，支付宝，云计算|娱乐)*P(娱乐)=`56/121x15/121x0/121x60/90`=0

**注**：

属于娱乐类别的概率为0，这是明显合理的

解决方法：拉普拉斯平滑系数 `P(F1|C)=(Ni+α)/(N+αm)` 

α为指定的系数一般为1，m为训练文档中统计出的特征词个数

例如：

科技：P(影院，支付宝，云计算|科技)*P(科技)=`9/100x21/100x64/100x31/90`

娱乐：P(影院，支付宝，云计算|娱乐)*P(娱乐)=`57/121x16/121x1/121x61/90`

## 朴素贝叶斯实现`API`

**`sklearn`朴素贝叶斯实现`API`**：`sklearn.naive_bayes.MultinomialNB`

**语法**：1.`sklearn.naive_bayes.MultinomialNB(alpha=1.0)`    

朴素贝叶斯分类

alpha:拉普拉斯平滑系数

## 朴素贝叶斯算法特点

训练集影响结果准确性

不需要调参

优点：发源于古典数学理论，有稳定的分类效率；对缺失数据不太敏感，算法也比较简单，常用于文本分类；分类准确度高，速度快

缺点：由于训练集中的词固定，准确率不能提升；由于使用样本属性独立性的假设，所以如果样本属性有关联时其效果不好