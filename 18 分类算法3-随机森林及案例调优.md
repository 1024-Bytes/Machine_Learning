# 随机森林的原理

## 随机森林介绍

随机森林是一种集成学习方法。

集成学习是通过建立几个模型组合来解决单一预测问题。它的工作原理是生成多个分类器或模型，各自独立地学习和做出预测。这些预测最后结合成单预测，因此优于任何一个单分类的做出预测。

## 随机森林建立多个决策树过程

1.用N来表示训练用例（样本）的个数，M表示特征数目。

2.一次随机选出一个样本，重复N次（有可能出现重复的样本，采取bootstrap抽样，是随机不放回的抽样）

3.随机去选出m个特征，m<<M，建立决策树。

# 随机森林`API`

`class sklearn.ensemble.RandomForestClassifier(n_estimators=10,criterion='gini',max_depth=None,bootstrap=True,random_state=None)`

随机森林分类器

`n_estimators`：`integer`，`optional(default=10)`  森林里的树木数量

`criteria`：`string`，可选`(default="gini")`  分割特征的测量方法

`max_depth`：`integer`或`None`，可选（默认=无）树的最大深度 5,8,15,25,30

`max_features="auto"`，每个决策树的最大特征数量

*If "auto",then 'max_features=sqrt(n_features)'*

*If "sqrt",then 'max_features=sqrt(n_features)'(same as "auto")*

*If "`log2`",then 'max_features=`log2`(n_features)'*

*If None,then 'max_features=n_features'*

`bootstrap`：`boolean`，`optional(default=True)`是否在构建树时使用放回抽样

# 随机森林案例及调优

**代码**：

```python
import pandas as pd
from sklearn.model_selection import train_test_split,GridSearchCV
from sklearn.feature_extraction import DictVectorizer
from sklearn.ensemble import RandomForestClassifier
def random_forest():
    """
    随机森林预测泰坦尼克号乘客能否幸存
    :return: None
    """
    # 获取数据
    titanic = pd.read_csv('./datasets/train.csv')
    # 处理数据，找出特征值和目标值
    x = titanic[['Age', 'Pclass', 'Sex']]
    y = titanic['Survived']
    # 处理缺失值
    x['Age'].fillna(x['Age'].mean(), inplace=True)
    # 分割数据集
    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25)
    # 特征工程处理数据（特征：类别-->one_hot编码）
    dict = DictVectorizer(sparse=False)
    x_train = dict.fit_transform(x_train.to_dict(orient="records"))
    x_test = dict.fit_transform(x_test.to_dict(orient="records"))
    # 随机森林进行预测（超参数调优）
    rf = RandomForestClassifier()
    param = {"n_estimators": [120, 200, 300, 500 ,800 ,1200], "max_depth": [5, 8, 15, 25, 30]}
    # 网格搜索与交叉验证
    gc = GridSearchCV(rf, param_grid=param, cv=2)
    gc.fit(x_train, y_train)
    print("准确率为:", '\n', gc.score(x_test, y_test))
    print("查看选择的参数模型:", '\n', gc.best_params_)
    return None


if __name__ == "__main__":
    random_forest()
```

**运行结果**：

```python
准确率为: 
 0.8251121076233184
查看选择的参数模型: 
 {'max_depth': 5, 'n_estimators': 1200}
```



# 随机森林的优点

1.在当前所有算法中，具有极好的准确率

2.能够有效地运行在大数据集上

3.能够处理具有高维度特征的输入样本，而不需要降维

4.能够评估各个特征在分类问题上的重要性