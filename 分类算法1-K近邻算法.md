如果一个样本在特征空间中的k个最相似(即特征空间中最邻近)的样本中的大多数属于某一个类别，该样本也属于这个类别。

**欧式距离**：两个样本的距离

**公式**：假设`a(a1,a2,a3),b(b1,b2,b3)`  , 样本间距离：`[(a1-b1)²+ (a2-b2)² + (a3-b3)² ]½`

**K-近邻算法需要做标准化处理**

# `sklearn k-近邻算法API及使用方法`

`sklearn.neighbors.KNeighbnorsClassfier(n_neighbors=5,algorithm='auto')`

1.`n_neighbors`:int,可选(默认=5)，`k_neighbors`查询默认使用的邻居数

2.`algorithm`:{'`auto`','`ball_tree`','`kd_tree`','`brute`'},可选用于算最近邻居的算法。'`ball_tree`'将会使用`BallTree`,'`kd_tree`'将使`KDTree`，'`auto`'将尝试根据传递给fit方法的值来决定最合适的算法(不同实现方式影响效率)

# `sklearn k-近邻算法的优缺点`

**k取值**：k值很小时，容易受异常点影响;k值取很大，容易受K值数量(类别)波动

**性能问题**：数据过多时运行起来比较费时

**优点**：无需估计参数，无需训练

**缺点**：对测试样本分类时的计算量大，内存开销大；必须指定K值，K值选择不当则分类精度不能保证

**使用场景**：小数据场景，几千~几千万样本，具体场景具体业务去测试

