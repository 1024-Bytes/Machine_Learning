# 字典特征抽取

对字典数据进行特征值化

```python
from sklearn.feature_extraction import DictVectorizer
#导入字典特征处理包
def dictvec():
    """
    字典数据抽取：
    return:None
    """
    dict = DictVectorizer()
    # 实例化               
  data=dict.fit_transform([{'city':'北京','temperature':30},{'city':'上海','temperature':35},{'city':'深圳','temperature':37}])
    # 调用fit_transform处理实例化后的字典
    print(data)
    #打印处理结果
    return None
if __name__=="__main__":
        dictvec()
        #调用函数
```

下面是运行结果：

```python
 #(0, 1)	1.0
 #(0, 3)	30.0
 #(1, 0)	1.0
 #(1, 3)	35.0
 #(2, 2)	1.0
 #(2, 3)	37.0
```

**注意**：这里的输出值是一个sparse矩阵，d但当`dict = DictVectorizer(sparse=False)`时，输出值就会像下面这样：

```python
#[[ 0.  1.  0. 30.]
 #[ 1.  0.  0. 35.]
 #[ 0.  0.  1. 37.]]
```

这里0. 1. 0. 以及下面的类似的值称为one-hot编码

引用`data.get_feature_names()`方法时就会得到这样的输出值（返回类别名称）:

```python
['city=上海', 'city=北京', 'city=深圳', 'temperature']
```

# `tf-idf`分析问题

**`tf`**:term frequency,词的频率

**`idf`**：inverse document frequency,逆文档频率（log(总文档数量/该词出现的文档数量)）

**重要性程度**=`tf*idf`

**类**:`sklearn.feature_extraction.text.TfidfVectorizer`

**`TfidfVectorizer`语法**：

`TfidfVectorizer(stop_words=None,...)`其中stop_words表示可忽略的词

`TfidfVectorizer.fit_ttransform(X)`其中X是文本或包含文本字符串的可迭代对象，返回值为sparse矩阵

`TfidfVectorizer.get_feature_names()`和前面一样返回一个单词列表，也就是涉及到的词汇

代码：

```python
def cutword():
    """
    将长字符串进行分词化处理，
    并将列表转化为可迭代字符串
    return:可迭代字符串
    """
    import jieba
    #导入分词包
    c1=jieba.lcut('欢迎仪式后，习近平主席在人民大会堂同马克龙总统会谈。')
    c2=jieba.lcut('11月6日，国家主席习近平在北京人民大会堂同法国总统马克龙共同出席中法经济峰会闭幕式。')
    c3=jieba.lcut('国家主席习近平和夫人彭丽媛5日晚在上海豫园会见法国总统马克龙和夫人布丽吉特。')
    #选择精确模式切分
    con1 = ' '.join(c1)
    con2 = ' '.join(c2)
    con3 = ' '.join(c3)
    #将列表转化成字符串
    return con1,con2,con3
    #返回字符串
from sklearn.feature_extraction.text import TfidfVectorizer
#导入文本及字符串特征处理包
def tfidfvec():
    """
    字符串数据抽取：
    return:None
    """
    c1,c2,c3=cutword()
    #获取分词化处理后的可迭代对象
    text = TfidfVectorizer()
    # 实例化
    data=text.fit_transform([c1,c2,c3])
    # 调用fit_transform处理实例化后的字符串
    print(text.get_feature_names())
    #打印类别名称
    print(data)
    #打印处理后的特征值
    return None
if __name__=="__main__":
       tfidfvec()  
```

运行结果：

```python
Building prefix dict from the default dictionary ...
Loading model from cache C:\Users\win10\AppData\Local\Temp\jieba.cache
Loading model cost 1.746 seconds.
Prefix dict has been built succesfully.
['11', '上海', '主席', '习近平', '人民大会堂', '会见', '会谈', '共同', '出席', '北京人民大会堂', '吉特', '国家', '夫人', '峰会', '布丽', '彭丽媛', '总统', '日晚', '欢迎仪式', '法国', '经济', '豫园', '闭幕式', '马克']
  (0, 6)	0.47698559825514686
  (0, 16)	0.28171537816186876
  (0, 23)	0.28171537816186876
  (0, 4)	0.47698559825514686
  (0, 2)	0.28171537816186876
  (0, 3)	0.28171537816186876
  (0, 18)	0.47698559825514686
  (1, 22)	0.32355668514323677
  (1, 13)	0.32355668514323677
  (1, 20)	0.32355668514323677
  (1, 8)	0.32355668514323677
  (1, 7)	0.32355668514323677
  (1, 19)	0.2460731988716326
  (1, 9)	0.32355668514323677
  (1, 11)	0.2460731988716326
  (1, 0)	0.32355668514323677
  (1, 16)	0.19109779046865402
  (1, 23)	0.19109779046865402
  (1, 2)	0.19109779046865402
  (1, 3)	0.19109779046865402
  (2, 10)	0.2716417787137587
  (2, 14)	0.2716417787137587
  (2, 5)	0.2716417787137587
  (2, 21)	0.2716417787137587
  (2, 1)	0.2716417787137587
  (2, 17)	0.2716417787137587
  (2, 15)	0.2716417787137587
  (2, 12)	0.5432835574275174
  (2, 19)	0.2065905744017726
  (2, 11)	0.2065905744017726
  (2, 16)	0.160436010426408
  (2, 23)	0.160436010426408
  (2, 2)	0.160436010426408
  (2, 3)	0.160436010426408
```

**注**：使用`toarray()`可以将矩阵形式的结果转化成数组形式。

代码：

```python
def cutword():
    """
    将长字符串进行分词化处理，
    并将列表转化为可迭代字符串
    return:可迭代字符串
    """
    import jieba
    #导入分词包
    c1=jieba.lcut('欢迎仪式后，习近平主席在人民大会堂同马克龙总统会谈。')
    c2=jieba.lcut('11月6日，国家主席习近平在北京人民大会堂同法国总统马克龙共同出席中法经济峰会闭幕式。')
    c3=jieba.lcut('国家主席习近平和夫人彭丽媛5日晚在上海豫园会见法国总统马克龙和夫人布丽吉特。')
    #选择精确模式切分
    con1 = ' '.join(c1)
    con2 = ' '.join(c2)
    con3 = ' '.join(c3)
    #将列表转化成字符串
    return con1,con2,con3
    #返回字符串
from sklearn.feature_extraction.text import TfidfVectorizer
#导入文本及字符串特征处理包
def tfidfvec():
    """
    字符串数据抽取：
    return:None
    """
    c1,c2,c3=cutword()
    #获取分词化处理后的可迭代对象
    text = TfidfVectorizer()
    # 实例化
    data=text.fit_transform([c1,c2,c3])
    # 调用fit_transform处理实例化后的字符串
    print(text.get_feature_names())
    #打印类别名称
    print(data.toarray())
    #打印处理后的特征值
    return None
if __name__=="__main__":
       tfidfvec()
```

运行结果：

```python
Building prefix dict from the default dictionary ...
Loading model from cache C:\Users\win10\AppData\Local\Temp\jieba.cache
Loading model cost 1.711 seconds.
Prefix dict has been built succesfully.
['11', '上海', '主席', '习近平', '人民大会堂', '会见', '会谈', '共同', '出席', '北京人民大会堂', '吉特', '国家', '夫人', '峰会', '布丽', '彭丽媛', '总统', '日晚', '欢迎仪式', '法国', '经济', '豫园', '闭幕式', '马克']
[[0.         0.         0.28171538 0.28171538 0.4769856  0.
  0.4769856  0.         0.         0.         0.         0.
  0.         0.         0.         0.         0.28171538 0.
  0.4769856  0.         0.         0.         0.         0.28171538]
 [0.32355669 0.         0.19109779 0.19109779 0.         0.
  0.         0.32355669 0.32355669 0.32355669 0.         0.2460732
  0.         0.32355669 0.         0.         0.19109779 0.
  0.         0.2460732  0.32355669 0.         0.32355669 0.19109779]
 [0.         0.27164178 0.16043601 0.16043601 0.         0.27164178
  0.         0.         0.         0.         0.27164178 0.20659057
  0.54328356 0.         0.27164178 0.27164178 0.16043601 0.27164178
  0.         0.20659057 0.         0.27164178 0.         0.16043601]]
```

**这里是对特征处理原理的猜测：特征处理时`TfidfVectorizer`类是根据符号(逗号、句号)及空格来区别不同词的，所以分词化处理后的字符串不同词间要用空格隔开，而英文各词之间则本身就是用空格隔开的，所以不需要分词化处理，也验证了猜测的正确性。**

# 文本特征抽取

可用于文本分类

类：`sklearn.feature_extraction.text.CountVectorizer()`

方法同前面一样

代码：

```python
from sklearn.feature_extraction.text import CountVectorizer
#导入文本特征处理包
def tfidfvec():
    """
    文本数据抽取：
    return:None
    """
    text = CountVectorizer()
    # 实例化
    data=text.fit_transform(['life is short,i like python','life is too long,i dislike python'])
    # 调用fit_transform处理实例化后的文本
    print(text.get_feature_names())
    #打印类别名称
    print(data.toarray())
    #打印处理后的特征值
    return None
if __name__=="__main__":
       tfidfvec()
```

运行结果：

```python
['dislike', 'is', 'life', 'like', 'long', 'python', 'short', 'too']
[[0 1 1 1 0 1 1 0]
 [1 1 1 0 1 1 0 1]]
```

**关键点**:

1.统计所有文章当中所有的词，重复的只看做一次(对应第一个列表——词的列表)

2.对每篇文章，在词的列表里面进行统计每个词出现的次数(对应第二个列表——数组)

3.单个字母不统计(对应第一个列表中没有`'i'`)，由于单个字母没有办法作为分类依据，所以不会被统计进去

而对于中文文本的特征处理，只需增加分词化处理，而且单个汉字也不被记入

**对比**：对文本的特征处理使用`tf-idf`分析重要性方法要较`count`方法精确

